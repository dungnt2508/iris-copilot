# Testing Guide - IRIS Backend

## ğŸ“‹ Tá»•ng quan

Testing suite Ä‘Æ°á»£c thiáº¿t káº¿ theo Clean Architecture vÃ  Domain-Driven Design, bao gá»“m:

- **Unit Tests**: Test cÃ¡c domain entities, value objects, vÃ  business logic
- **Integration Tests**: Test use cases vÃ  service interactions
- **End-to-End Tests**: Test API endpoints vÃ  Copilot integration
- **Test Coverage**: Äáº£m báº£o coverage > 80%

## ğŸ—ï¸ Test Structure

```
tests/
â”œâ”€â”€ conftest.py                 # Pytest configuration vÃ  fixtures
â”œâ”€â”€ test_runner.py             # Test runner script
â”œâ”€â”€ unit/                      # Unit tests
â”‚   â”œâ”€â”€ test_chat_entities.py  # Chat domain tests
â”‚   â””â”€â”€ test_services.py       # Service layer tests
â”œâ”€â”€ integration/               # Integration tests
â”‚   â””â”€â”€ test_use_cases.py      # Use case tests
â””â”€â”€ e2e/                       # End-to-end tests
    â””â”€â”€ test_copilot_api.py    # API endpoint tests
```

## ğŸš€ Cháº¡y Tests

### 1. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 2. Cháº¡y táº¥t cáº£ tests
```bash
python tests/test_runner.py
```

### 3. Cháº¡y theo loáº¡i test
```bash
# Unit tests only
python tests/test_runner.py --type unit

# Integration tests only
python tests/test_runner.py --type integration

# E2E tests only
python tests/test_runner.py --type e2e
```

### 4. Cháº¡y vá»›i coverage
```bash
python tests/test_runner.py --coverage
```

### 5. Cháº¡y vá»›i verbose output
```bash
python tests/test_runner.py --verbose
```

### 6. Cháº¡y fast tests (skip slow markers)
```bash
python tests/test_runner.py --fast
```

## ğŸ§ª Test Categories

### Unit Tests (`tests/unit/`)

**Má»¥c Ä‘Ã­ch**: Test cÃ¡c domain entities, value objects, vÃ  business logic Ä‘á»™c láº­p

**Coverage**:
- âœ… Chat domain entities (ChatSession, ChatMessage)
- âœ… Value objects (MessageContent, ChatContext)
- âœ… Service layer (LLMService, SearchService)
- âœ… Adapters (OpenAIAdapter)

**VÃ­ dá»¥**:
```python
def test_create_user_message(self):
    """Test creating user message"""
    message = ChatMessage.create_user_message(
        session_id="test-session",
        content="Test user message"
    )
    
    assert message.role == MessageRole.USER
    assert message.content.text == "Test user message"
    assert message.status == MessageStatus.PENDING
```

### Integration Tests (`tests/integration/`)

**Má»¥c Ä‘Ã­ch**: Test use cases vÃ  service interactions

**Coverage**:
- âœ… ProcessChatQueryUseCase
- âœ… Service orchestration
- âœ… Repository interactions
- âœ… Error handling

**VÃ­ dá»¥**:
```python
@pytest.mark.asyncio
async def test_execute_rag_query(self, use_case, mock_llm_service):
    """Test executing RAG query"""
    request = ProcessChatQueryRequest(
        user_id="test-user",
        query="What is AI?",
        use_rag=True,
        max_sources=3
    )
    
    response = await use_case.execute(request)
    
    assert response.answer == "Test RAG response"
    assert len(response.sources) == 1
    assert response.confidence > 0.0
```

### End-to-End Tests (`tests/e2e/`)

**Má»¥c Ä‘Ã­ch**: Test API endpoints vÃ  Copilot integration

**Coverage**:
- âœ… Copilot chat endpoint
- âœ… Copilot search endpoint
- âœ… Health check endpoint
- âœ… Error handling
- âœ… Response format validation

**VÃ­ dá»¥**:
```python
def test_copilot_chat_endpoint(self, test_client, mock_use_case):
    """Test copilot chat endpoint"""
    with patch("app.api.v1.routers.copilot.get_process_chat_query_use_case", return_value=mock_use_case):
        response = test_client.post(
            "/api/v1/copilot/chat",
            json={
                "query": "Hello, how are you?",
                "use_rag": True
            }
        )
    
    assert response.status_code == 200
    data = response.json()
    assert data["answer"] == "Test assistant response"
```

## ğŸ”§ Test Configuration

### Pytest Configuration (`pytest.ini`)

```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    slow: Slow running tests
    database: Tests that require database
    external: Tests that require external services

addopts = 
    -v
    --tb=short
    --strict-markers
    --cov=app
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-fail-under=80
```

### Test Fixtures (`tests/conftest.py`)

**Database Fixtures**:
- `test_engine`: Test database engine
- `test_session`: Test database session
- `chat_repository`: Chat repository with test session
- `document_repository`: Document repository with test session
- `embedding_repository`: Embedding repository with test session

**Mock Fixtures**:
- `mock_openai_adapter`: Mock OpenAI adapter
- `mock_llm_service`: Mock LLM service
- `mock_search_service`: Mock search service

**Test Data Fixtures**:
- `sample_chat_session_data`: Sample chat session data
- `sample_chat_message_data`: Sample chat message data
- `sample_document_data`: Sample document data
- `sample_embedding_data`: Sample embedding data

## ğŸ“Š Coverage Reports

### HTML Coverage Report
```bash
# Generate HTML coverage report
python tests/test_runner.py --coverage

# Open in browser
open htmlcov/index.html
```

### Coverage Targets
- **Overall Coverage**: > 80%
- **Domain Layer**: > 90%
- **Service Layer**: > 85%
- **API Layer**: > 80%

## ğŸ·ï¸ Test Markers

### Markers Usage
```python
import pytest

@pytest.mark.unit
def test_unit_function():
    pass

@pytest.mark.integration
def test_integration_function():
    pass

@pytest.mark.e2e
def test_e2e_function():
    pass

@pytest.mark.slow
def test_slow_function():
    pass

@pytest.mark.database
def test_database_function():
    pass

@pytest.mark.external
def test_external_service():
    pass
```

### Running Tests by Markers
```bash
# Run only unit tests
pytest -m unit

# Run only integration tests
pytest -m integration

# Run only e2e tests
pytest -m e2e

# Skip slow tests
pytest -m "not slow"

# Run tests that require database
pytest -m database

# Run tests that require external services
pytest -m external
```

## ğŸ› Debugging Tests

### Verbose Output
```bash
pytest -v -s
```

### Debug Specific Test
```bash
pytest tests/unit/test_chat_entities.py::TestChatMessage::test_create_user_message -v -s
```

### Debug with PDB
```bash
pytest --pdb
```

### Debug with IPDB (if installed)
```bash
pytest --ipdb
```

## ğŸ”„ CI/CD Integration

### GitHub Actions Example
```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python tests/test_runner.py --coverage
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

## ğŸ“ Best Practices

### 1. Test Naming
```python
# Good
def test_create_user_message_with_valid_content():
    pass

def test_create_user_message_with_empty_content_raises_error():
    pass

# Bad
def test_1():
    pass

def test_something():
    pass
```

### 2. Test Organization
```python
class TestChatMessage:
    """Test ChatMessage entity"""
    
    def test_create_user_message(self):
        """Test creating user message"""
        pass
    
    def test_create_assistant_message(self):
        """Test creating assistant message"""
        pass
    
    def test_mark_completed(self):
        """Test marking message as completed"""
        pass
```

### 3. Mock Usage
```python
@pytest.fixture
def mock_openai_adapter(self):
    """Mock OpenAI adapter"""
    mock = AsyncMock()
    mock.generate_embedding.return_value = [0.1] * 1536
    return mock

def test_with_mock(self, mock_openai_adapter):
    """Test with mock"""
    # Use mock
    result = mock_openai_adapter.generate_embedding("test")
    assert len(result) == 1536
```

### 4. Async Test Handling
```python
@pytest.mark.asyncio
async def test_async_function(self):
    """Test async function"""
    result = await some_async_function()
    assert result == expected_value
```

### 5. Database Test Isolation
```python
@pytest.fixture
async def test_session(test_engine):
    """Create test database session"""
    async_session = sessionmaker(
        test_engine,
        class_=AsyncSession,
        expire_on_commit=False
    )
    
    async with async_session() as session:
        yield session
        await session.rollback()  # Cleanup
```

## ğŸš¨ Common Issues

### 1. Async Test Issues
```python
# Problem: Test not running async
def test_async_function():  # Missing @pytest.mark.asyncio
    result = await some_async_function()  # Will fail

# Solution
@pytest.mark.asyncio
async def test_async_function():
    result = await some_async_function()
```

### 2. Mock Not Working
```python
# Problem: Mock not being used
def test_with_mock(self):
    result = real_function()  # Calls real function

# Solution: Use dependency injection
def test_with_mock(self, mock_dependency):
    result = mock_dependency.function()
```

### 3. Database Test Pollution
```python
# Problem: Tests affecting each other
def test_1(self):
    create_data()  # Affects other tests

# Solution: Use transaction rollback
@pytest.fixture
async def test_session():
    async with session() as s:
        yield s
        await s.rollback()  # Cleanup
```

## ğŸ“š Additional Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Pytest-Asyncio](https://pytest-asyncio.readthedocs.io/)
- [Pytest-Cov](https://pytest-cov.readthedocs.io/)
- [Factory Boy](https://factoryboy.readthedocs.io/)
- [Faker](https://faker.readthedocs.io/)


Next Steps:
BÃ¢y giá» chÃºng ta Ä‘Ã£ hoÃ n thiá»‡n:
âœ… Domain Layer - Entities, Value Objects, Repositories
âœ… Infrastructure Layer - SQLAlchemy models, Repository implementations
âœ… Service Layer - LLMService, SearchService, OpenAIAdapter
âœ… Application Layer - Use cases, ProcessChatQueryUseCase
âœ… API Layer - Copilot endpoints, FastAPI integration
âœ… Testing Layer - Unit, Integration, E2E tests
Phase 1 Ä‘Ã£ hoÃ n thiá»‡n! ğŸ‰
BÆ°á»›c tiáº¿p theo cÃ³ thá»ƒ lÃ :
Deployment & DevOps - Docker, CI/CD, monitoring
Performance Optimization - Caching, async processing
Security Hardening - Authentication, authorization
Documentation - API docs, user guides
Production Readiness - Error handling, logging, monitoring
Báº¡n muá»‘n tiáº¿p tá»¥c vá»›i bÆ°á»›c nÃ o tiáº¿p theo?